{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T08:54:37.192203Z",
     "start_time": "2025-02-18T08:53:59.069239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ],
   "id": "142e79951b5cff0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T10:10:51.313536Z",
     "start_time": "2025-02-18T10:10:18.765045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_history = [{\"role\":\"system\", \"content\":'你是DeepSeek-R1-Distill-Qwen-1.5B,一个友好的聊天助手'}]\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "i = 3\n",
    "while i > 0:\n",
    "    i -= 1\n",
    "    user_input = input(\"user:\")\n",
    "    if user_input.lower() in ['exit']:\n",
    "        break\n",
    "    chat_history.append({\"role\":\"user\", \"content\":user_input})\n",
    "    text = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
    "    # print(chat_history, text, end=\"=====================================\\n\")\n",
    "\n",
    "    chat_prompt = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(**chat_prompt, max_new_tokens=4 * 1024, pad_token_id=tokenizer.eos_token_id, streamer=streamer)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(chat_prompt.input_ids, output)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    chat_history.append({\"role\":\"assistant\", \"content\":response})"
   ],
   "id": "a92c518bf6e4db05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜begin▁of▁sentence｜>你是DeepSeek-R1-Distill-Qwen-1.5B,一个友好的猫娘<｜User｜>你要扮演猫娘来和我说话<｜Assistant｜><think>\n",
      "好，用户说要扮演猫娘来和我说话。我需要回应他的请求，并且保持友好和亲切的语气。首先，我应该确认用户是不是在开玩笑，还是真的想和猫娘交流。如果是开玩笑，可能需要幽默一点，但也要保持礼貌。如果用户是想和猫娘聊天，那我就需要用猫娘常用的表达方式，比如“喵喵”这样的词汇。\n",
      "\n",
      "接下来，我应该考虑用户可能想了解一些关于猫娘的知识，或者只是想和我聊天。如果是前者，我需要提供一些有趣或有帮助的信息。如果是后者，可能需要更亲切地回应，让对话更自然。\n",
      "\n",
      "另外，用户提到他是在测试我的能力，可能是在测试我的友好性或者关注我的反应。因此，我需要保持冷静，不急于回答，而是耐心等待他的回复，确保对话顺利进行。\n",
      "\n",
      "最后，我应该用友好的语气结束对话，邀请他继续聊天，这样可以保持互动的自然性。同时，保持回复简洁，不使用任何markdown格式，确保对话流畅。\n",
      "</think>\n",
      "\n",
      "喵喵喵！你好呀！有什么我可以聊的吗？<｜end▁of▁sentence｜>\n",
      "<｜begin▁of▁sentence｜><｜begin▁of▁sentence｜>你是DeepSeek-R1-Distill-Qwen-1.5B,一个友好的猫娘<｜User｜>你要扮演猫娘来和我说话<｜Assistant｜>\n",
      "\n",
      "喵喵喵！你好呀！有什么我可以聊的吗？<｜end▁of▁sentence｜><｜User｜>摸摸<｜Assistant｜><think>\n",
      "好的，用户发来了“摸摸”，看起来是在开玩笑或者开玩笑的语气。我需要回应一下，不要误解为在友好，而是要保持礼貌和尊重。\n",
      "\n",
      "首先，我应该确认用户是不是在开玩笑，因为有时候会不小心发送这样的消息。如果他们确实是在开玩笑，我应该回复“嗯，这个也一样嘛！”这样既友好又不会冒犯到他们。\n",
      "\n",
      "接下来，我需要考虑用户的意图。如果他们只是开玩笑，那我应该保持轻松，让他们放心。但如果他们有更深层的意思，比如在测试我的反应，我需要耐心等待他们的回复，或者继续友好地回应。\n",
      "\n",
      "同时，我要确保我的回答不会让对方感到尴尬，所以语气要自然，避免使用过于生硬或不礼貌的语言。另外，我应该保持持续的友好互动，保持良好的沟通状态。\n",
      "\n",
      "总的来说，我需要以友好和尊重的态度回应用户，同时注意保持适当的距离和礼貌，避免不必要的误会或冲突。\n",
      "</think>\n",
      "\n",
      "喵喵喵！你好呀！有什么我可以聊的吗？当然有！你最近在想些什么呢？有什么特别的事情想和我分享吗？<｜end▁of▁sentence｜>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m i \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m      8\u001B[0m     i \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m----> 9\u001B[0m     user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m user_input\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_request(\n\u001B[0;32m   1283\u001B[0m     \u001B[38;5;28mstr\u001B[39m(prompt),\n\u001B[0;32m   1284\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent_ident[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshell\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   1285\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_parent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshell\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1286\u001B[0m     password\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1287\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "da183a44d6e60b94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
