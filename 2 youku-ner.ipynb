{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 数据预处理",
   "id": "7888023552b6dae6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T09:17:31.434350Z",
     "start_time": "2024-12-18T09:17:31.431749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification"
   ],
   "id": "efabdb07dee4951e",
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T09:17:31.513172Z",
     "start_time": "2024-12-18T09:17:31.508311Z"
    }
   },
   "source": [
    "def read_data(file_path):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        label = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:\n",
    "                word, tag = parts\n",
    "                sentence.append(word)\n",
    "                label.append(tag)\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def handle_data(file_path):\n",
    "    \"\"\"\n",
    "    处理数据，将数据转化为模型可以接受的格式\n",
    "    \"\"\"\n",
    "    sentences, labels = read_data(file_path)\n",
    "\n",
    "    tokenized_inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    tokenized_labels = []\n",
    "\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        encoded = tokenizer(sentence, is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=32 , return_tensors='pt')\n",
    "\n",
    "\n",
    "        word_ids = encoded.word_ids()\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_id]])\n",
    "\n",
    "        tokenized_inputs[\"input_ids\"].append(encoded[\"input_ids\"])\n",
    "        tokenized_inputs[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
    "        tokenized_labels.append(label_ids)\n",
    "\n",
    "    return torch.cat(tokenized_inputs[\"input_ids\"]), torch.cat(tokenized_inputs[\"attention_mask\"]), torch.tensor(tokenized_labels)\n",
    "\n",
    "file_path = './data/youku/train.txt'"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T09:17:32.679236Z",
     "start_time": "2024-12-18T09:17:31.515270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id2label = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-MISC', 4: 'I-MISC', 5: 'B-TELEVISION', 6: 'I-TELEVISION'}\n",
    "label2id = {label: i for i, label in enumerate(id2label.values())}\n",
    "\n",
    "input_ids, attention_mask, labels = handle_data(file_path)\n",
    "\n",
    "train_dataset = TensorDataset(input_ids, attention_mask, labels)"
   ],
   "id": "80000ec2bcace6b1",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 模型训练",
   "id": "b2a57985a9371597"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T09:17:32.727678Z",
     "start_time": "2024-12-18T09:17:32.724356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs, batch_size = 8, 64\n",
    "lr = 5e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "e67f854467b8d86e",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T09:20:33.062316Z",
     "start_time": "2024-12-18T09:17:44.886289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained( 'bert-base-chinese', num_labels=7)\n",
    "model.to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(trainer, T_max=len(train_loader) * epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    bar.set_description(f\"epoch: {epoch + 1}\")\n",
    "    for batch in bar:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        trainer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        l = loss(outputs.logits.view(-1, 7), labels.view(-1))\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += l.item()\n",
    "        bar.set_postfix(loss=total_loss / (batch[0].shape[0] * (epoch + 1)))\n",
    "\n"
   ],
   "id": "ec4213f54d5dd6e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "epoch: 1: 100%|██████████| 126/126 [00:21<00:00,  5.96it/s, loss=51.8] \n",
      "epoch: 2: 100%|██████████| 126/126 [00:20<00:00,  6.01it/s, loss=13.4] \n",
      "epoch: 3: 100%|██████████| 126/126 [00:21<00:00,  5.97it/s, loss=5.99]  \n",
      "epoch: 4: 100%|██████████| 126/126 [00:20<00:00,  6.07it/s, loss=2.81]  \n",
      "epoch: 5: 100%|██████████| 126/126 [00:20<00:00,  6.08it/s, loss=1.4]   \n",
      "epoch: 6: 100%|██████████| 126/126 [00:20<00:00,  6.07it/s, loss=0.795] \n",
      "epoch: 7: 100%|██████████| 126/126 [00:20<00:00,  6.05it/s, loss=0.557]  \n",
      "epoch: 8: 100%|██████████| 126/126 [00:21<00:00,  5.97it/s, loss=0.413]  \n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 模型评估",
   "id": "f214d7e4c9d734a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T09:22:35.615377Z",
     "start_time": "2024-12-18T09:22:33.999758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试\n",
    "file_path = './data/youku/test.txt'\n",
    "input_ids, attention_mask, true_labels = handle_data(file_path)\n",
    "test_dataset = TensorDataset(input_ids, attention_mask, true_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "model.eval()\n",
    "preds = []\n",
    "total_acc = 0\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids, attention_mask, true_labels = batch\n",
    "    input_ids, attention_mask, true_labels = input_ids.to(device), attention_mask.to(device), true_labels.to(device)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy().tolist())\n",
    "    # 实体标注的评估\n",
    "    # i = 3\n",
    "    # print(outputs.logits.argmax(dim=-1)[i])\n",
    "    # print(true_labels[i])\n",
    "    # print(attention_mask[i])\n",
    "    right = (outputs.logits.argmax(dim=-1) == true_labels).sum(-1)\n",
    "    all = (attention_mask.sum(-1) - 2)\n",
    "    total_acc += (right / all).sum().item() / len(batch[0])\n",
    "\n",
    "print(total_acc / len(test_loader))"
   ],
   "id": "5d1224658a41c0fc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9367983786434662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T09:49:15.885229Z",
     "start_time": "2024-12-18T09:49:15.042825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 随机展示一些效果\n",
    "import random\n",
    "# random.seed(42)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "for i in range(10):\n",
    "    idx = random.randint(0, len(input_ids) - 1 )\n",
    "    input_id = input_ids[idx]\n",
    "    mask = attention_mask[idx]\n",
    "\n",
    "    # print(input_id )\n",
    "    # print( mask)\n",
    "\n",
    "    pred= model(input_ids = input_id.unsqueeze(0), attention_mask = mask.unsqueeze(0))\n",
    "    pred = pred.logits.argmax(dim=-1).cpu().numpy().tolist()\n",
    "\n",
    "    valid_len = mask.sum()\n",
    "    for i in range(1, valid_len - 1):\n",
    "        # 实体命名开头\n",
    "        if id2label[pred[0][i]][0] == \"B\" : print()\n",
    "\n",
    "        if id2label[pred[0][i]] == 'O':  print(tokenizer.decode(input_id[i]))\n",
    "        else: print(tokenizer.decode(input_id[i]) , id2label[pred[0][i]])"
   ],
   "id": "ebb532b0d5ccdfc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "网 B-TELEVISION\n",
      "球 I-TELEVISION\n",
      "王 I-TELEVISION\n",
      "子 I-TELEVISION\n",
      "1\n",
      "6\n",
      "\n",
      "江 B-MISC\n",
      "阴 I-MISC\n",
      "外\n",
      "语\n",
      "培\n",
      "训\n",
      "\n",
      "江 B-MISC\n",
      "阴 I-MISC\n",
      "学\n",
      "外\n",
      "语\n",
      "哪\n",
      "里\n",
      "好\n",
      "女\n",
      "神\n",
      "喊\n",
      "你\n",
      "学\n",
      "外\n",
      "语\n",
      "\n",
      "爱 B-TELEVISION\n",
      "情 I-TELEVISION\n",
      "保 I-TELEVISION\n",
      "卫 I-TELEVISION\n",
      "战 I-TELEVISION\n",
      "\n",
      "爱 B-TELEVISION\n",
      "要 I-TELEVISION\n",
      "有 I-TELEVISION\n",
      "你 I-TELEVISION\n",
      "搞\n",
      "笑\n",
      "视\n",
      "频\n",
      "\n",
      "西 B-TELEVISION\n",
      "游 I-TELEVISION\n",
      "搞\n",
      "笑\n",
      "视\n",
      "频\n",
      "搞\n",
      "笑\n",
      "电\n",
      "影\n",
      ",\n",
      "恶\n",
      "搞\n",
      "\n",
      "西 B-TELEVISION\n",
      "游 I-TELEVISION\n",
      "记 I-TELEVISION\n",
      "\n",
      "孙 B-MISC\n",
      "悟 I-MISC\n",
      "空 I-MISC\n",
      "1\n",
      "采\n",
      "访\n",
      "\n",
      "郑 B-PER\n",
      "伊 I-PER\n",
      "健 I-PER\n",
      ",\n",
      "\n",
      "北 B-MISC\n",
      "京 I-MISC\n",
      "电\n",
      "影\n",
      "节\n",
      ",\n",
      "闭\n",
      "幕\n",
      "式\n",
      ",\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      ",\n",
      "标\n",
      "清\n",
      "三\n",
      "哥\n",
      "\n",
      "苗 B-PER\n",
      "僑 I-PER\n",
      "偉 I-PER\n",
      "（\n",
      "\n",
      "使 B-TELEVISION\n",
      "徒 I-TELEVISION\n",
      "行 I-TELEVISION\n",
      "者 I-TELEVISION\n",
      "剪\n",
      "輯\n",
      "片\n",
      "）\n",
      "【\n",
      "\n",
      "奇 B-PER\n",
      "怪 I-PER\n",
      "君 I-PER\n",
      "-\n",
      "[UNK]\n",
      "家\n",
      "】\n",
      ",\n",
      "[UNK]\n",
      "i\n",
      "n\n",
      "e\n",
      "c\n",
      "r\n",
      "a\n",
      "f\n",
      "t\n",
      ",\n",
      "我\n",
      "的\n",
      "世\n",
      "界\n",
      ",\n",
      "神\n",
      "奇\n",
      "宝\n",
      "贝\n",
      "口\n",
      "袋\n",
      "\n",
      "柯 B-TELEVISION\n",
      "南 I-TELEVISION\n",
      "剧\n",
      "场\n",
      "\n",
      "九 B-TELEVISION\n",
      "水 I-TELEVISION\n",
      "平 I-TELEVISION\n",
      "线 I-TELEVISION\n",
      "上 I-TELEVISION\n",
      "的 I-TELEVISION\n",
      "阴 I-TELEVISION\n",
      "谋 I-TELEVISION\n",
      "【\n",
      "粤\n",
      "语\n",
      "】\n",
      "[UNK]\n",
      "\n",
      "南 B-PER\n",
      "阳 I-PER\n",
      "任 I-PER\n",
      "国 I-PER\n",
      "熙 I-PER\n",
      "珍\n",
      "藏\n",
      ",\n",
      ",\n",
      "港\n",
      "台\n",
      "经\n",
      "典\n",
      "恐\n",
      "怖\n",
      "鬼\n",
      "片\n",
      "《\n",
      "\n",
      "阴 B-TELEVISION\n",
      "阳 I-TELEVISION\n",
      "界 I-TELEVISION\n",
      "》\n",
      "{\n",
      "国\n",
      "语\n",
      "}\n",
      "这\n",
      "\n",
      "西 B-TELEVISION\n",
      "游 I-TELEVISION\n",
      "记 I-TELEVISION\n",
      "，\n",
      "让\n",
      "人\n",
      "蛋\n",
      "疼\n",
      "啊\n",
      "！\n",
      "采\n",
      "访\n",
      "\n",
      "郑 B-PER\n",
      "伊 I-PER\n",
      "健 I-PER\n",
      ",\n",
      "\n",
      "北 B-MISC\n",
      "京 I-MISC\n",
      "电\n",
      "影\n",
      "节\n",
      ",\n",
      "闭\n",
      "幕\n",
      "式\n",
      ",\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      ",\n",
      "标\n",
      "清\n"
     ]
    }
   ],
   "execution_count": 123
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
