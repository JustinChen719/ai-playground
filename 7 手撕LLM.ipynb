{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:38:38.288375Z",
     "start_time": "2025-02-15T05:38:38.284514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "id": "92d5fa718f863395",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:38:38.304004Z",
     "start_time": "2025-02-15T05:38:38.300795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_model = 16\n",
    "d_head = 4\n",
    "content_length = 16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "fe38237e92bdd4e4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 模型定义",
   "id": "6cb7fc4d5b623d8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 FFN",
   "id": "37eda875f24469c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:38:38.313455Z",
     "start_time": "2025-02-15T05:38:38.308008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ffn(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(ffn, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4 * d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) # (batch, seq_len, d_model) -> (batch, seq_len, 4 * d_model)\n",
    "        x = self.relu(x)    # (batch, seq_len, 4 * d_model)\n",
    "        x = self.linear2(x) # (batch, seq_len, 4 * d_model) -> (batch, seq_len, d_model)\n",
    "        x = self.dropout(x) # (batch, seq_len, d_model)\n",
    "        return x"
   ],
   "id": "1389f07c54a2b236",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 Attention",
   "id": "7aca21b8b46c7357"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:38:38.325186Z",
     "start_time": "2025-02-15T05:38:38.320521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, d_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.wq = nn.Linear(d_model, d_head)\n",
    "        self.wk = nn.Linear(d_model, d_head)\n",
    "        self.wv = nn.Linear(d_model, d_head)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(content_length, content_length)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        B, T, D = k.shape\n",
    "        q, k, v = self.wq(q), self.wk(k), self.wv(v)\n",
    "        output = (q @ k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.d_head))\n",
    "        output = output.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        output = F.softmax(output, dim=-1)\n",
    "        # output = self.dropout(output)\n",
    "        output = output @ v\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.attns = nn.ModuleList([Attention(self.d_model, self.d_head, dropout) for _ in range(n_head)])\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        x = torch.cat([attn(q, k, v) for attn in self.attns], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ],
   "id": "3f69f2b832fecdcd",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:38:38.347137Z",
     "start_time": "2025-02-15T05:38:38.339208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(1, content_length, d_model)\n",
    "attn = MultiHeadAttention(d_model, d_head)\n",
    "attn(x, x, x)"
   ],
   "id": "d0ea6ff74822bc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.5958,  0.1640, -0.0561, -0.0000, -0.2798,  1.1937,\n",
       "          -0.1650, -0.3092, -1.2333, -0.1858, -0.1201,  0.1896, -0.1828,\n",
       "          -0.5437,  0.2806],\n",
       "         [ 0.0411,  0.2474, -0.1662, -0.0161, -0.1543, -0.0492,  0.4396,\n",
       "           0.0109,  0.1336, -0.6234, -0.1889, -0.3460,  0.2228,  0.1669,\n",
       "          -0.0702, -0.1018],\n",
       "         [-0.0071,  0.1386, -0.1260, -0.0817, -0.0188, -0.0161,  0.2137,\n",
       "          -0.0269,  0.3236, -0.4561, -0.0000, -0.5035,  0.2920,  0.1162,\n",
       "           0.1887, -0.3010],\n",
       "         [ 0.0302,  0.1596,  0.0136, -0.1083, -0.0831, -0.0985,  0.2624,\n",
       "           0.0679,  0.3374, -0.5200, -0.0913, -0.3921,  0.5072,  0.1286,\n",
       "           0.1298, -0.2699],\n",
       "         [-0.0924,  0.2128, -0.1262,  0.0000, -0.1432,  0.0271,  0.3496,\n",
       "           0.0000,  0.2732, -0.5393, -0.0162, -0.3434,  0.4345,  0.1667,\n",
       "          -0.0000, -0.0000],\n",
       "         [-0.0613,  0.0000, -0.0601,  0.1259, -0.1188, -0.0490,  0.0000,\n",
       "           0.0394,  0.2111, -0.4838, -0.0226, -0.2394,  0.3857,  0.1259,\n",
       "           0.0530, -0.2716],\n",
       "         [ 0.0022,  0.0624, -0.0275,  0.0848, -0.0138, -0.0320,  0.2107,\n",
       "           0.0528,  0.1824, -0.3217, -0.0000, -0.0000,  0.3524, -0.0019,\n",
       "           0.1589, -0.2632],\n",
       "         [ 0.0832,  0.1120,  0.1362, -0.0000, -0.0000, -0.0834,  0.3255,\n",
       "           0.0764,  0.1764, -0.3389, -0.0000, -0.2558,  0.3621,  0.0177,\n",
       "           0.1704, -0.1634],\n",
       "         [ 0.0554,  0.1595,  0.0823, -0.0000, -0.0000, -0.0992,  0.2119,\n",
       "           0.0857,  0.0757, -0.3440, -0.1287, -0.3144,  0.4371,  0.1200,\n",
       "           0.1738, -0.0882],\n",
       "         [-0.0407,  0.0740, -0.0134,  0.1886, -0.0743, -0.0392,  0.0000,\n",
       "           0.1118,  0.1565, -0.3762, -0.0000, -0.3368,  0.4479,  0.2050,\n",
       "           0.1068, -0.2222],\n",
       "         [-0.1004,  0.2244, -0.1327,  0.1140, -0.0772, -0.0455,  0.0000,\n",
       "           0.1010,  0.0000, -0.0000, -0.0830, -0.1647,  0.0000,  0.2161,\n",
       "           0.0525, -0.0743],\n",
       "         [ 0.0204,  0.1272,  0.0389,  0.0778, -0.0084, -0.0077,  0.2007,\n",
       "           0.0423,  0.1145, -0.3300, -0.1245, -0.2859,  0.3957,  0.0156,\n",
       "           0.1208, -0.1483],\n",
       "         [-0.0790,  0.1934, -0.1093,  0.1160, -0.0000, -0.0495,  0.1641,\n",
       "           0.0936,  0.0782, -0.4590, -0.1058, -0.3225,  0.3826,  0.3211,\n",
       "           0.0506, -0.0990],\n",
       "         [-0.1238,  0.1968, -0.1762,  0.3032, -0.0835,  0.0355,  0.0566,\n",
       "           0.1445,  0.0905, -0.3556, -0.1663, -0.1359,  0.2915,  0.2856,\n",
       "           0.0908, -0.2227],\n",
       "         [-0.1032,  0.1176, -0.0654,  0.1274, -0.0537, -0.0315,  0.2182,\n",
       "           0.1210,  0.1460, -0.3756, -0.0778, -0.3121,  0.4269,  0.0000,\n",
       "           0.0000, -0.1753],\n",
       "         [-0.1266,  0.0000, -0.1649,  0.2144, -0.1278,  0.0360,  0.1975,\n",
       "           0.0424,  0.0954, -0.0000, -0.0000, -0.1395,  0.3198,  0.1398,\n",
       "           0.1343, -0.2735]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
