{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:45:15.756056Z",
     "start_time": "2025-02-15T05:45:14.428388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "id": "92d5fa718f863395",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:45:15.779952Z",
     "start_time": "2025-02-15T05:45:15.759567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_model = 16\n",
    "d_head = 4\n",
    "content_length = 16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "fe38237e92bdd4e4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 模型定义",
   "id": "6cb7fc4d5b623d8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 FFN",
   "id": "37eda875f24469c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:45:15.843226Z",
     "start_time": "2025-02-15T05:45:15.840404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ffn(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(ffn, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4 * d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) # (batch, seq_len, d_model) -> (batch, seq_len, 4 * d_model)\n",
    "        x = self.relu(x)    # (batch, seq_len, 4 * d_model)\n",
    "        x = self.linear2(x) # (batch, seq_len, 4 * d_model) -> (batch, seq_len, d_model)\n",
    "        x = self.dropout(x) # (batch, seq_len, d_model)\n",
    "        return x"
   ],
   "id": "1389f07c54a2b236",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 Attention",
   "id": "7aca21b8b46c7357"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:45:15.861142Z",
     "start_time": "2025-02-15T05:45:15.856253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, d_head, dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        # 这里使用的另一种高效的实现，不是计算后再切分，而是直接进过线性层变换维度为d_head，最后直接拼接\n",
    "        self.wq = nn.Linear(d_model, d_head)\n",
    "        self.wk = nn.Linear(d_model, d_head)\n",
    "        self.wv = nn.Linear(d_model, d_head)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(content_length, content_length)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        B, T, D = k.shape\n",
    "        q, k, v = self.wq(q), self.wk(k), self.wv(v)\n",
    "        output = (q @ k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.d_head))\n",
    "        output = output.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        output = F.softmax(output, dim=-1)\n",
    "        # output = self.dropout(output)\n",
    "        output = output @ v\n",
    "        return output\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout = 0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.attns = nn.ModuleList([Attention(self.d_model, self.d_head, dropout) for _ in range(n_head)])\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([attn(x, x, x) for attn in self.attns], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ],
   "id": "3f69f2b832fecdcd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T05:45:15.890502Z",
     "start_time": "2025-02-15T05:45:15.877557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(1, content_length, d_model)\n",
    "attn = MultiHeadSelfAttention(d_model, d_head)\n",
    "attn(x)"
   ],
   "id": "d0ea6ff74822bc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4184,  0.3011,  0.2741, -0.3853,  0.0799,  0.1975,  0.0000,\n",
       "          -0.5492, -0.4010, -0.0000,  0.0000,  0.2386, -0.4940,  0.4856,\n",
       "          -0.1477,  0.0871],\n",
       "         [-0.2510, -0.0513,  0.0573, -0.0000,  0.4182,  0.0616,  0.1208,\n",
       "          -0.6447, -0.4309, -0.1921,  0.2582,  0.1586, -0.0000,  0.1088,\n",
       "          -0.0524,  0.1730],\n",
       "         [-0.2748,  0.0510, -0.3411, -0.3255,  0.0705,  0.1340,  0.0234,\n",
       "          -0.2968, -0.3339, -0.0000, -0.1511, -0.0755, -0.0826,  0.0950,\n",
       "          -0.0640,  0.0668],\n",
       "         [-0.2100, -0.0920, -0.2085, -0.3110,  0.0000,  0.2229, -0.0199,\n",
       "          -0.3194, -0.0000, -0.1728, -0.0101, -0.0200, -0.0992,  0.2268,\n",
       "          -0.0783,  0.0793],\n",
       "         [-0.2838, -0.0073, -0.1365, -0.2856,  0.3233,  0.2559,  0.0061,\n",
       "          -0.4334, -0.3212, -0.1337,  0.0867, -0.1098, -0.2197,  0.1594,\n",
       "          -0.0000,  0.0928],\n",
       "         [-0.3385,  0.0000,  0.1276, -0.1334,  0.3864,  0.2349,  0.0918,\n",
       "          -0.4840, -0.2200, -0.2542,  0.0410, -0.1178, -0.3399,  0.1190,\n",
       "          -0.1569,  0.0463],\n",
       "         [-0.2649,  0.0000,  0.1588, -0.1850,  0.3166,  0.1940,  0.0444,\n",
       "          -0.4050, -0.1598, -0.3475, -0.0361, -0.0214, -0.2433,  0.1912,\n",
       "          -0.0845, -0.0295],\n",
       "         [-0.3352,  0.0741,  0.2189, -0.1783,  0.0000,  0.0826, -0.1287,\n",
       "          -0.5281, -0.0778, -0.2742,  0.0643,  0.1199, -0.2135,  0.0921,\n",
       "          -0.1452, -0.0072],\n",
       "         [-0.3263,  0.0611,  0.1810, -0.1350,  0.3944,  0.0972, -0.1179,\n",
       "          -0.4825, -0.1298, -0.2819,  0.0697,  0.0554, -0.1726,  0.1130,\n",
       "          -0.1666,  0.0741],\n",
       "         [-0.3006,  0.0027,  0.1009, -0.1128,  0.4038,  0.1309, -0.0964,\n",
       "          -0.4400, -0.2004, -0.2473,  0.0635,  0.0177, -0.1557,  0.0000,\n",
       "          -0.1774,  0.1548],\n",
       "         [-0.2672, -0.0420,  0.1426, -0.1053,  0.4199,  0.0912, -0.0834,\n",
       "          -0.4823, -0.0000, -0.2392,  0.0555,  0.0511, -0.1535,  0.1857,\n",
       "          -0.1965,  0.0000],\n",
       "         [-0.0000, -0.0054,  0.0765, -0.0561,  0.3484,  0.0526, -0.1165,\n",
       "          -0.4061, -0.1847, -0.2732,  0.0551,  0.0228, -0.0604,  0.1618,\n",
       "          -0.1840,  0.1625],\n",
       "         [-0.0000, -0.0393,  0.0443, -0.0020,  0.3241,  0.0303, -0.1146,\n",
       "          -0.3610, -0.1928, -0.2733,  0.0439, -0.0169, -0.0224,  0.1248,\n",
       "          -0.1279,  0.1604],\n",
       "         [-0.2093, -0.0299,  0.0498, -0.0022,  0.3029,  0.0823, -0.1285,\n",
       "          -0.3058, -0.2200, -0.2668, -0.0062, -0.1047, -0.0124,  0.1364,\n",
       "          -0.1490,  0.1163],\n",
       "         [-0.2354, -0.0311,  0.1316, -0.0399,  0.3271,  0.0983, -0.1419,\n",
       "          -0.3564, -0.2034, -0.2568,  0.0650, -0.0453, -0.0761,  0.1508,\n",
       "          -0.1446,  0.0981],\n",
       "         [-0.2418, -0.0276,  0.1365, -0.0515,  0.3183,  0.1123, -0.1416,\n",
       "          -0.3848, -0.1688, -0.2661, -0.0015, -0.0335, -0.1340,  0.1889,\n",
       "          -0.2159,  0.0437]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
