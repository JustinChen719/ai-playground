{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 数据预处理",
   "id": "adce63022dc5ee72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:12:00.371359Z",
     "start_time": "2024-12-31T11:11:42.089993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ],
   "id": "50609ea2e44bb63f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对数据进行分词、编码",
   "id": "c0e4aec12e0b1414"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:12:00.774530Z",
     "start_time": "2024-12-31T11:12:00.375370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ],
   "id": "13a9baa29671fed8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-31T11:12:00.828952Z",
     "start_time": "2024-12-31T11:12:00.784798Z"
    }
   },
   "source": "tokenized_dataset = datasets.map(tokenize_function, batched=True, num_proc=1, remove_columns=\"text\")",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对数据进行合并再切分，这样可以不是用padding来填充，有效利用空闲空间。",
   "id": "370cb001ac04f196"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:12:00.840503Z",
     "start_time": "2024-12-31T11:12:00.836653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # 拼接所有文本\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # 我们将余数对应的部分去掉。但如果模型支持的话，可以添加padding，您可以根据需要定制此部件。\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # 通过max_len进行分割。\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "id": "66d071b2eb3def33",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:12:00.861385Z",
     "start_time": "2024-12-31T11:12:00.849528Z"
    }
   },
   "cell_type": "code",
   "source": "lm_dataset = tokenized_dataset.map(group_texts, batched=True, batch_size=1000)",
   "id": "fae57e46261d0fe0",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 加载模型",
   "id": "cdaceede255e29b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:12:01.502862Z",
     "start_time": "2024-12-31T11:12:00.868825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ],
   "id": "c6d402f6f0ec4c6e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 训练",
   "id": "3ce0ebc0dceef128"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:32:47.946045Z",
     "start_time": "2024-12-31T11:18:13.662400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir= \"output\",\n",
    "    eval_strategy= \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    eval_delay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "68fb7e33aa78c685",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenzhen\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7002' max='7002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7002/7002 14:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.760600</td>\n",
       "      <td>3.666432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.648700</td>\n",
       "      <td>3.647044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.603700</td>\n",
       "      <td>3.643931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7002, training_loss=3.693270469862745, metrics={'train_runtime': 873.737, 'train_samples_per_second': 64.094, 'train_steps_per_second': 8.014, 'total_flos': 1829109916237824.0, 'train_loss': 3.693270469862745, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 检验效果",
   "id": "fbaea9c5599b23f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:43:23.464203Z",
     "start_time": "2024-12-31T11:43:23.282432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 微调以后的效果\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/causallm-distilgpt2\")\n",
    "text = \"Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input).logits\n",
    "tokenizer.decode(output[0].argmax(-1))"
   ],
   "id": "4024d15049f69a13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' , the 2010 , the , and was the first game in the seriesalkyria Chronicles to Iting a same name system the elements tactical @-@ time combat , the predecessor , the game is'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T11:43:25.188034Z",
     "start_time": "2024-12-31T11:43:24.337281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 原本模型输出效果\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "text = \"Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input).logits\n",
    "tokenizer.decode(output[0].argmax(-1))"
   ],
   "id": "fd0c29c4f95e52b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The the,. the. the was the first time in the series-ria Chronicles to Iting a same name system the and tactical-klife style,, the predecessor, the game is'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
