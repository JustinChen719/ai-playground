{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 数据预处理",
   "id": "3252ed8e6baa2a50"
  },
  {
   "cell_type": "code",
   "id": "9f95b5ed8c2ca87e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:27:43.794193Z",
     "start_time": "2024-12-20T11:27:39.073865Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:27:53.055354Z",
     "start_time": "2024-12-20T11:27:49.407621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = \"./data/hsk_1_4.txt\"\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "def handle_data(path):\n",
    "    lines = read_data(path)\n",
    "    '''\n",
    "    english:  A gust of wind blew the door shut.\n",
    "    hsk: 1\n",
    "    mandarin: 一阵大风吹来，把门关上了。\n",
    "    pinyin: yī zhèn dà fēng chuī lái， bǎ mén guān shàng le。\n",
    "    --\n",
    "    '''\n",
    "    input = []\n",
    "    target = []\n",
    "    for line in lines:\n",
    "        if \"english:\" in line:\n",
    "            input.append(\"translate English to Chinese: \" + line[8:].strip())\n",
    "        elif \"mandarin:\" in line:\n",
    "            target.append(line[9:].strip())\n",
    "    return input, target\n",
    "\n",
    "tokenizer_name = \"google/mt5-small\"\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name, local_files_only=True)\n",
    "\n",
    "input, target = handle_data(data_path)\n",
    "encoded_input = tokenizer.batch_encode_plus(input, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "encoded_target = tokenizer.batch_encode_plus(target, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "dataset = TensorDataset(encoded_input[\"input_ids\"], encoded_input[\"attention_mask\"], encoded_target)"
   ],
   "id": "9086102c0d678443",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 模型训练",
   "id": "6587fdd05570f7c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T07:39:02.176422Z",
     "start_time": "2024-12-20T07:39:02.084715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs, batch_size = 5, 16\n",
    "lr = 5e-5\n",
    "num_workers = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "e0255efe0482bcf4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T07:54:54.190600Z",
     "start_time": "2024-12-20T07:39:02.210525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, num_labels=len(tokenizer.get_vocab()), local_files_only=True)\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "max_norm = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    bar = tqdm(train_loader)\n",
    "    bar.set_description(f\"epoch: {epoch + 1}\")\n",
    "    for i, batch in enumerate(bar):\n",
    "        input_ids, attention_mask, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        trainer.zero_grad()\n",
    "        loss_value = outputs.loss\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        trainer.step()\n",
    "        bar.set_postfix(loss=loss_value.item())\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "epoch: 1:   0%|          | 0/1398 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "epoch: 1: 100%|██████████| 1398/1398 [03:06<00:00,  7.50it/s, loss=0.896]\n",
      "epoch: 2: 100%|██████████| 1398/1398 [03:08<00:00,  7.43it/s, loss=0.461]\n",
      "epoch: 3: 100%|██████████| 1398/1398 [03:11<00:00,  7.29it/s, loss=0.721]\n",
      "epoch: 4: 100%|██████████| 1398/1398 [03:12<00:00,  7.27it/s, loss=0.618]\n",
      "epoch: 5: 100%|██████████| 1398/1398 [03:10<00:00,  7.35it/s, loss=0.384]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 效果验证",
   "id": "379ffe8131861451"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:47:54.211517Z",
     "start_time": "2024-12-20T11:47:49.443154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  transformers import T5Config\n",
    "\n",
    "sentences = [\"A black swan is rare.\",\n",
    "             \"A breeder means a person who breeds animals.\",\n",
    "             \"A German scientist interrupted me and asked if I came from China.\",\n",
    "             \"How old are you.\"\n",
    "             ]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 使用训练完毕的模型参数进行预测输出结果\n",
    "model_saved = \"models/en_zh_translation-mt5-epoch5.pth\"\n",
    "config = T5Config.from_pretrained(model_name, num_labels=len(tokenizer.get_vocab()), local_files_only=True)\n",
    "model = T5ForConditionalGeneration(config)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_saved, local_model_only=True)\n",
    "model.load_state_dict(torch.load(model_saved, weights_only= True))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"en: {sentence}\")\n",
    "    input_ids = tokenizer(\"translate English to Chinese: \" + sentence, return_tensors=\"pt\", max_length=128, padding=\"max_length\", truncation=True).input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    outputs = model.generate(input_ids)\n",
    "    # print(outputs)\n",
    "    print(f\"zh: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")\n"
   ],
   "id": "85937767b3a969d4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: A black swan is rare.\n",
      "zh: 黑白的斑斓是稀罕的。\n",
      "\n",
      "en: A breeder means a person who breeds animals.\n",
      "zh: 驯养者是驯养动物的驯养者\n",
      "\n",
      "en: A German scientist interrupted me and asked if I came from China.\n",
      "zh: 一位德国科学家向我问我是否来自中国。\n",
      "\n",
      "en: How old are you.\n",
      "zh: 你怎样老?\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
